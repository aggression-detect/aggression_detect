# -*- coding: utf-8 -*-
"""C3D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KlLXEqqq1I_VDvtiNcyTb8unWY2ixYV-
"""

# -*- coding: utf-8 -*-
from math import floor
import numpy as np
import glob
#import cv2
import re
from numpy.random import seed
seed(1)
import os
import h5py
import gc
import re
from tensorflow.keras import Sequential
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten,
		 	  Activation, Dense, Dropout, ZeroPadding2D)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import BatchNormalization  
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
# import openvino

# get the work path
path = os.getcwd() 
#path = os.path.abspath(os.path.dirname(path)+os.path.sep+".")
os.chdir(path)
os.listdir(path)
print (path)

# name list of all dataset 
data_kind_list = ["Movie Dataset", "HockeyFights", "Surveillance Camera Fight Dataset", "youtube_fight"]
data_kind = data_kind_list[0]
data_path = path+"/video_image"+ "/"+ data_kind
print ("data path:",data_path)
os.listdir(data_path)

rate = 0.8 # training set : test set
fight_files = glob.glob(data_path+'/fight'+'/*mp4')
noFight_files = glob.glob(data_path+'/noFight'+'/*mp4')
total_files = fight_files + noFight_files

print ("fight files:", len(fight_files))
print ("noFight files:",len(noFight_files))
np.random.shuffle(fight_files)
np.random.shuffle(noFight_files)


test_file = fight_files[floor(len(fight_files)*rate):]+ noFight_files[floor(len(noFight_files)*rate):]
train_file = fight_files[:floor(len(fight_files)*rate)]+ noFight_files[:floor(len(noFight_files)*rate)]

# size for potical flow batch
L = 5
epoch = 1
batch_size = 2

print ("fight data = ",len(glob.glob(r"%s"%(data_path+"/fight/*.mp4"))))
print ("noFight data = ",len(glob.glob(r"%s"%(data_path+"/noFight/*.mp4"))))

def sort_key(s):
    if s:
        try:
            c = re.findall(r"\d+", s)[-1]

        except:
            c = -1
        return int(c)

def generator(list1, lits2):
    '''
    Auxiliar generator: returns the ith element of both given list with
         each call to next() 
    '''
    for x,y in zip(list1,lits2):
        yield x, y



def train_model(learning_rate=0.0001,num_features=4096):
    model = Sequential()

    model.add(ZeroPadding3D((1, 1, 1), input_shape=(16, 112, 112, 3)))
    model.add(Conv3D(64, (3, 3, 3), activation='relu', name='conv1a'))
    model.add(MaxPooling3D((1, 2, 2), strides=(1, 2, 2)))

    model.add(ZeroPadding3D((1, 1, 1), input_shape=(16, 56, 56, 3)))
    model.add(Conv3D(128, (3, 3, 3), activation='relu', name='conv2a'))
    model.add(MaxPooling3D((2, 2, 2), strides=(2, 2, 2)))

    model.add(ZeroPadding3D((1, 1, 1), input_shape=(8, 28, 28, 3)))
    model.add(Conv3D(256, (3, 3, 3), activation='relu', name='conv3a'))
    model.add(ZeroPadding3D((1, 1, 1), input_shape=(8, 28, 28, 3)))
    model.add(Conv3D(256, (3, 3, 3), activation='relu', name='conv3b'))
    model.add(MaxPooling3D((2, 2, 2), strides=(2, 2, 2)))


    model.add(ZeroPadding3D((1, 1, 1), input_shape=(4, 14, 14, 3)))
    model.add(Conv3D(512, (3, 3, 3), activation='relu', name='conv4a'))
    model.add(ZeroPadding3D((1, 1, 1), input_shape=(4, 14, 14, 3)))
    model.add(Conv3D(512, (3, 3, 3), activation='relu', name='conv4b'))
    model.add(MaxPooling3D((2, 2, 2), strides=(2, 2, 2)))

    model.add(ZeroPadding3D((1, 1, 1), input_shape=(2, 7, 7, 3)))
    model.add(Conv3D(512, (3, 3, 3), activation='relu', name='conv5a'))
    model.add(ZeroPadding3D((1, 1, 1), input_shape=(2, 7, 7, 3)))
    model.add(Conv3D(512, (3, 3, 3), activation='relu', name='conv5b'))
    model.add(MaxPooling3D((2, 2, 2), strides=(2, 2, 2)))

    model.add(Flatten())
    model.add(Dense(num_features, name='fc6', kernel_initializer='glorot_uniform'))

    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))
    model.add(Activation('relu'))
    model.add(Dropout(0.1))

    model.add(Dense(num_features, name='fc7', kernel_initializer='glorot_uniform'))
    model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    model.add(Dense(1, name='predictions',kernel_initializer='glorot_uniform'))
    model.add(Activation('sigmoid'))

    adam = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999,
        epsilon=1e-08)
    model.compile(optimizer=adam, loss='binary_crossentropy',
        metrics=['accuracy'])
    return model

model = train_model()
model.summary()

trainx = np.zeros((13,16,112,112,3))
trainy = np.zeros((13))

testx = np.zeros((20,16,112,112,3))
testy = np.zeros((20))

def data_generator(train_file,batch_size,L=16):
    count = 0
    trainx = np.zeros(shape=(0,L,224,224,3), dtype=np.float64)
    trainy = []
    while (1):
        for video_path in train_file:
            count +=1
            
            images = glob.glob(video_path + '/*.jpg')
            print (images)
            images.sort(key=sort_key)
            nb_stacks = len(images)-L+1
            stack = np.zeros(shape=(224,224,3,L,nb_stacks), dtype=np.float64)
            # print(video_path)
            # print(np.shape(x_images),np.shape(y_images))

            for i in range(len(images)):
                img = cv2.imread(images[i], cv2.IMREAD_COLOR)
                # Assign an image i to the jth stack in the kth position, but also
                # in the j+1th stack in the k+1th position and so on
                # (for sliding window) 
                for s in list(reversed(range(min(L,i+1)))):
                    if i-s < nb_stacks:
                        stack[:,:,:,s,i-s] = img
                del img
                gc.collect()                
           
            flow = np.transpose(flow, (4, 3, 0, 1, 2))
            # print (np.shape(flow))
            trainx = np.concatenate((trainx, flow), axis=0)
			
            if "noFight" in video_path:
                trainy += [0 for i in range(len(flow))]
            else:
                trainy += [1 for i in range(len(flow))]
            del flow
            gc.collect()
            if count % batch_size == 0:
                trainy =np.array(trainy)
                trainy.reshape((-1,1))
                yield trainx,trainy
                del trainx,trainy
                gc.collect()
                count = 0
                trainx = np.zeros(shape=(0,224,224,2*L), dtype=np.float64)
                trainy = []

model.fit(data_generator (train_file,batch_size), steps_per_epoch=len(train_file)//batch_size,
				epochs = epoch,validation_data=data_generator(test_file,batch_size),validation_steps= len(test_file)//batch_size, verbose = 2 )